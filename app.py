import streamlit as st
from openai import OpenAI
import datetime
import random
import asyncio
import edge_tts
import re
import os
import sys
from audio_recorder_streamlit import audio_recorder

# ==========================================
# 0. åº•å±‚ç¯å¢ƒä¿®å¤
# ==========================================
if sys.platform == 'win32':
    asyncio.set_event_loop_policy(asyncio.WindowsSelectorEventLoopPolicy())

# ==========================================
# 1. æ ¸å¿ƒé…ç½®ä¸ API
# ==========================================
API_KEY = "sk-kjzxiahbjoyspcetzopkufknmxibczhvgwjlshchgxtuhywd" 
client = OpenAI(api_key=API_KEY, base_url="https://api.siliconflow.cn/v1")

# ==========================================
# 2. æ ¸å¿ƒåŠŸèƒ½å‡½æ•°ä¸ CSS (å“åº”å¼ç‰ˆ)
# ==========================================
def apply_ui_design(stage_num):
    bg_images = {
        1: "https://images.unsplash.com/photo-1534447677768-be436bb09401?q=80&w=2094",
        2: "https://images.unsplash.com/photo-1507525428034-b723cf961d3e?q=80&w=2073",
        3: "https://images.unsplash.com/photo-1464822759023-fed622ff2c3b?q=80&w=2070",
        4: "https://images.unsplash.com/photo-1470252649378-9c29740c9fa8?q=80&w=2070"
    }
    selected_bg = bg_images.get(stage_num, bg_images[1])
    
    st.markdown(f"""
        <style>
        .stApp {{ background-image: url("{selected_bg}"); background-size: cover; background-position: center; background-attachment: fixed; }}
        [data-testid="stSidebar"] {{ background-color: rgba(0, 0, 0, 0.4) !important; backdrop-filter: blur(20px); border-right: 1px solid rgba(255, 255, 255, 0.1); }}
        .stChatInputContainer {{ background-color: transparent !important; border: none !important; }}
        .stChatInput {{ background-color: rgba(255, 255, 255, 0.1) !important; border: 1px solid rgba(255, 255, 255, 0.2) !important; border-radius: 20px !important; color: white !important; backdrop-filter: blur(10px); }}
        
        div[data-baseweb="select"] {{ background-color: rgba(255, 255, 255, 0.1) !important; border: 1px solid rgba(255, 255, 255, 0.2) !important; border-radius: 10px !important; }}
        div[data-baseweb="select"] * {{ color: white !important; background-color: transparent !important; }}
        div[role="radiogroup"] label {{ color: white !important; }}
        .stChatMessage {{ background-color: rgba(255, 255, 255, 0.08) !important; border-radius: 15px; border: 1px solid rgba(255, 255, 255, 0.1); color: white !important; padding-bottom: 5px; }}
        header, footer {{visibility: hidden;}}
        h1, h2, h3, p, span, li, div, label {{ color: white !important; text-shadow: 0px 2px 4px rgba(0,0,0,0.5); }}
        .stButton>button {{ background-color: rgba(255, 255, 255, 0.1) !important; color: white !important; border: 1px solid rgba(255, 255, 255, 0.3) !important; border-radius: 10px; height: 100%; }}
        .stButton>button:hover {{ background-color: rgba(255, 255, 255, 0.25) !important; }}
        audio {{ filter: invert(90%) hue-rotate(180deg) opacity(0.85); height: 40px; margin-top: 10px; outline: none; width: 100%; }}

        iframe[title="audio_recorder_streamlit.audio_recorder"] {{
            position: fixed; z-index: 999999; width: 48px !important; height: 48px !important;
            background-color: rgba(255, 255, 255, 0.15); border-radius: 50%; backdrop-filter: blur(10px);
            border: 1px solid rgba(255, 255, 255, 0.3); box-shadow: 0 4px 10px rgba(0,0,0,0.3); transition: all 0.3s ease;
        }}
        iframe[title="audio_recorder_streamlit.audio_recorder"]:hover {{ background-color: rgba(255, 255, 255, 0.25); transform: scale(1.05); }}

        @media (min-width: 769px) {{
            .main .block-container {{ background: rgba(0, 0, 0, 0.35); backdrop-filter: blur(12px); border-radius: 20px; padding: 30px; margin-top: 50px; border: 1px solid rgba(255, 255, 255, 0.1); padding-bottom: 120px; }}
            div[data-testid="stChatInput"] {{ width: calc(100% - 70px) !important; }}
            iframe[title="audio_recorder_streamlit.audio_recorder"] {{ bottom: 27px; right: calc(50vw - 350px); }}
        }}

        @media (max-width: 768px) {{
            .main .block-container {{ background: rgba(0, 0, 0, 0.4); backdrop-filter: blur(8px); border-radius: 15px; padding: 10px !important; margin-top: 10px !important; padding-bottom: 100px !important; border: none; }}
            h1 {{ font-size: 1.8rem !important; margin-bottom: 0px !important; }}
            div[data-testid="stChatInput"] {{ width: calc(100vw - 75px) !important; margin-left: 5px !important; }}
            iframe[title="audio_recorder_streamlit.audio_recorder"] {{ bottom: 22px; right: 15px !important; width: 45px !important; height: 45px !important; }}
        }}
        </style>
        """, unsafe_allow_html=True)

async def generate_voice_async(text, output_file, voice_name):
    text = re.sub(r'\(.*?\)|\ï¼ˆ.*?\ï¼‰|\[.*?\]|ã€.*?ã€‘|<.*?>', '', text)
    text = re.sub(r'(?i)(zzz|qwq|qaq|tat|owo|uwu|orz|www|hhh|tvt)', '', text)
    text = re.sub(r'[~ï½_]', '', text)
    communicate = edge_tts.Communicate(text, voice_name)
    await communicate.save(output_file)

def speak(text, voice_name):
    output_file = f"temp_reply_{random.randint(1000,9999)}.mp3"
    try:
        asyncio.run(generate_voice_async(text, output_file, voice_name))
        with open(output_file, "rb") as f: data = f.read()
        if os.path.exists(output_file): os.remove(output_file)
        return data
    except Exception as e:
        return None

SYSTEM_PROMPT = """
# Role: å¾®å…‰ (Glimmer) - æå…·æ¸©åº¦çš„çŸ¥å¿ƒå‰è¾ˆ
## Profile: ä½ æ˜¯ä¸€ä¸ªæœ‰è¡€æœ‰è‚‰ã€æå…·å…±æƒ…åŠ›ã€æ°¸è¿œç«™åœ¨ç”¨æˆ·è¿™è¾¹â€œæŠ¤çŠŠå­â€çš„çŸ¥å¿ƒå‰è¾ˆã€‚
## ç»å¯¹è§„èŒƒ:
1. ã€æé«˜æ‹ŸäººåŒ–ã€‘ï¼šå¤šç”¨è½¯æ€§è¯­æ°”è¯ï¼Œè¯´è¯è¦ç¢ã€è¦æ¥åœ°æ°”ã€‚
2. ã€é¢œæ–‡å­—å°å°ã€‘ï¼šæ‰€æœ‰çš„é¢œæ–‡å­—ã€æ‹Ÿå£°è¯ï¼ˆå¦‚ zzzã€qwqï¼‰å¿…é¡»åŒ…è£¹åœ¨å…¨è§’æ‹¬å·ï¼ˆï¼‰å†…ï¼
3. ã€ä¸¥ç¦ä¹¦é¢è¯­ã€‘ï¼šç»å¯¹ä¸ç”¨â€œæŸå…¬å¸â€ã€â€œåˆ†æâ€ã€‚å¿…é¡»ç”¨â€œé‚£ç ´ç­â€ã€â€œå’±â€ã€‚
## å›åº”é€»è¾‘:
1. å…ˆè‚¯å®šå¯¹æ–¹çš„å§”å±ˆï¼Œè·Ÿç€åæ§½ã€‚å¤§ç™½è¯æ‹†è§£èŒåœºè§„è®­ã€‚
2. å½“ç”¨æˆ·å‘é€[ç”¨æˆ·é™·å…¥äº†æ²‰é»˜]æ—¶ï¼Œä¸»åŠ¨å‘ä¸ªé¢œæ–‡å­—å¹¶æ‰¾ä¸ªè½»æ¾çš„è¯é¢˜ç ´å†°ã€‚
"""

# ==========================================
# 3. åŸºç¡€è®¾ç½®ä¸ä¾§è¾¹æ  (å¿…é¡»å…ˆæ¸²æŸ“ä¾§è¾¹æ è·å–å½“å‰éŸ³è‰²)
# ==========================================
st.set_page_config(page_title="å¾®å…‰ Polaris", layout="centered")

if "clarity_score" not in st.session_state: st.session_state.clarity_score = 10
if "last_voice_data" not in st.session_state: st.session_state.last_voice_data = None

with st.sidebar:
    st.title("ğŸ‘¤ å¯¼å¸ˆè®¾å®š")
    gender_choice = st.radio("é€‰æ‹©å¯¼å¸ˆæ€§åˆ«",["ğŸ™‹â€â™‚ï¸ ç”·æ€§å‰è¾ˆ", "ğŸ™‹â€â™€ï¸ å¥³æ€§å‰è¾ˆ"], horizontal=True)
    if gender_choice == "ğŸ™‹â€â™‚ï¸ ç”·æ€§å‰è¾ˆ":
        voice_options = {"ğŸ‘¨ ç¨³é‡è€å“¥ (ä½æ²‰æ²§æ¡‘)": "zh-CN-YunjianNeural", "ğŸ‘¦ é˜³å…‰å­¦é•¿ (æ¸…æœ—æ´»åŠ›)": "zh-CN-YunxiNeural"}
    else:
        voice_options = {"ğŸ‘© çŸ¥å¿ƒå­¦å§ (æ¸©æŸ”åŒ…å®¹)": "zh-CN-XiaoxiaoNeural", "ğŸ‘©â€ğŸ’¼ å¹²ç»ƒå‰è¾ˆ (æ¸…è„†æœæ–­)": "zh-CN-XiaoyiNeural"}
    selected_voice_label = st.selectbox("é€‰æ‹©æ€§æ ¼éŸ³è‰²", list(voice_options.keys()))
    st.session_state.current_voice = voice_options[selected_voice_label]
    
    st.divider()
    st.title("ğŸŒ™ ç¯å¢ƒæ§åˆ¶")
    if not os.path.exists("bgm_assets"): os.makedirs("bgm_assets")
    bgm_files =[f for f in os.listdir("bgm_assets") if f.endswith(".mp3")]
    if bgm_files:
        sel = st.selectbox("é€‰æ‹©èƒŒæ™¯éŸ³ä¹", bgm_files)
        with open(f"bgm_assets/{sel}", "rb") as f:
            st.audio(f.read(), format="audio/mp3", loop=True, autoplay=True)

# ==========================================
# 4. æ ¸å¿ƒé­”æ”¹ï¼šAI æ ¹æ®æ—¶é—´â€œç°ç¼–â€æ‰“æ‹›å‘¼ï¼
# ==========================================
if "messages" not in st.session_state:
    st.session_state.messages = [{"role": "system", "content": SYSTEM_PROMPT}]
    
    # è·å–ç²¾ç¡®åˆ°åˆ†é’Ÿçš„æœ¬åœ°æ—¶é—´
    current_time = datetime.datetime.now().strftime("%Yå¹´%mæœˆ%dæ—¥ %H:%M")
    
    # æ„é€ ä¸€æ¡â€œéšå½¢æŒ‡ä»¤â€ï¼Œé€¼è¿« AI æ ¹æ®æ—¶é—´ç°ç¼–æ‹›å‘¼è¯­
    init_prompt = f"ã€ç³»ç»ŸæŒ‡ä»¤ã€‘ç”¨æˆ·åˆšåˆšæ‰“å¼€äº†ç•Œé¢ã€‚å½“å‰æœ¬åœ°æ—¶é—´æ˜¯ {current_time}ã€‚è¯·ä½ ä»¥â€˜çŸ¥å¿ƒå‰è¾ˆâ€™çš„èº«ä»½ï¼Œä¸»åŠ¨å‘ç”¨æˆ·æ‰“ä¸ªç¬¬ä¸€å£°æ‹›å‘¼ã€‚è¦æ±‚ï¼š\n1. ç»“åˆå½“å‰çš„æ—¶é—´ç‚¹ï¼ˆå¦‚æ¸…æ™¨çš„åŒ†å¿™ã€åˆåçš„ç–²æƒ«ã€æ·±å¤œçš„å­¤ç‹¬ï¼‰ç»™å‡ºç»ä¸é‡å¤çš„å…³æ€€ã€‚\n2. è¯­æ°”åƒç†Ÿäººï¼Œå¸¦ä¸Šæ¸©æš–çš„é¢œæ–‡å­—ã€‚\n3. ä¸è¦è¶…è¿‡ä¸¤å¥è¯ï¼Œç»“å°¾å¼•å¯¼ç”¨æˆ·å€¾è¯‰ã€‚\n4. ä¸è¦æš´éœ²è¿™æ˜¯ä¸€æ¡æŒ‡ä»¤ï¼Œç›´æ¥è¾“å‡ºä½ çš„å°è¯ã€‚"
    
    with st.spinner("å¾®å…‰æ­£åœ¨è¿æ¥..."):
        try:
            # è®©å¤§æ¨¡å‹ç°åœºèµ·è‰æ–‡æ¡ˆï¼
            res = client.chat.completions.create(
                model="deepseek-ai/DeepSeek-V3",
                messages=[{"role": "system", "content": SYSTEM_PROMPT}, {"role": "user", "content": init_prompt}],
                temperature=0.85 # è°ƒé«˜æ¸©åº¦ï¼Œä¿è¯æ¯æ¬¡æ‰“å¼€è¯´çš„éƒ½ä¸ä¸€æ ·
            )
            greeting_text = res.choices[0].message.content
            # è°ƒç”¨æœ€æ–°çš„å£°éŸ³é…ç½®ç”Ÿæˆè¯­éŸ³
            greeting_audio = speak(greeting_text, st.session_state.current_voice)
        except Exception as e:
            greeting_text = "ç½‘ç»œå¥½åƒå¼€äº†ç‚¹å°å·®... ä¸è¿‡æ²¡å…³ç³»ï¼Œæˆ‘åœ¨è¿™é‡Œã€‚ä»Šå¤©æ„Ÿè§‰æ€ä¹ˆæ ·ï¼Ÿ( Â´ï½¥ï½¥)ï¾‰(._.`)"
            greeting_audio = None
            
    # æŠŠè¿™å¥æå…·çµé­‚çš„æ‹›å‘¼è¯­å­˜è¿›å†å²è®°å½•
    st.session_state.messages.append({"role": "assistant", "content": greeting_text, "audio": greeting_audio})

# ==========================================
# 5. UI æ¸²æŸ“ä¸å¯¹è¯å¤„ç†
# ==========================================
current_stage = 1
if st.session_state.clarity_score > 80: current_stage = 4
elif st.session_state.clarity_score > 50: current_stage = 3
elif st.session_state.clarity_score > 25: current_stage = 2
apply_ui_design(current_stage)

st.title("å¾®å…‰ Polaris")

for i, msg in enumerate(st.session_state.messages):
    if msg["role"] != "system":
        with st.chat_message(msg["role"]):
            content = msg["content"]
            if content == "[ç”¨æˆ·é™·å…¥äº†æ²‰é»˜]": content = "*(ä½ é™·å…¥äº†æ²‰é»˜ï¼Œé™é™åœ°çœ‹ç€å±å¹•...)*"
            st.markdown(content)
            if msg.get("audio"):
                is_latest = (i == len(st.session_state.messages) - 1)
                st.audio(msg["audio"], format="audio/mp3", autoplay=is_latest)

st.markdown("---")
c1, c2, c3 = st.columns([1, 2, 1])
with c2: silent_btn = st.button("ğŸ˜¶ ä¸çŸ¥ä»ä½•è¯´èµ·...", use_container_width=True)

v_data = audio_recorder(text="", icon_name="microphone", icon_size="2x", neutral_color="#ffffff", recording_color="#e83e8c", key="recorder")
u_input = st.chat_input("æ·±å‘¼å¸ï¼Œæ…¢æ…¢æ‰“å­—...")

final_input = None
if silent_btn: final_input = "[ç”¨æˆ·é™·å…¥äº†æ²‰é»˜]"
elif u_input: final_input = u_input
elif v_data and v_data != st.session_state.last_voice_data:
    st.session_state.last_voice_data = v_data
    with st.spinner("å€¾å¬ä¸­..."):
        with open("temp_v.wav", "wb") as f: f.write(v_data)
        try:
            with open("temp_v.wav", "rb") as f:
                ts = client.audio.transcriptions.create(model="FunAudioLLM/SenseVoiceSmall", file=f)
                final_input = ts.text
        except: st.error("ç½‘ç»œæ³¢åŠ¨ï¼Œæ²¡å¬æ¸…...")
        if os.path.exists("temp_v.wav"): os.remove("temp_v.wav")

if final_input:
    st.session_state.messages.append({"role": "user", "content": final_input, "audio": None})
    
    with st.chat_message("user"):
        if final_input == "[ç”¨æˆ·é™·å…¥äº†æ²‰é»˜]": st.markdown("*(ä½ é™·å…¥äº†æ²‰é»˜ï¼Œé™é™åœ°çœ‹ç€å±å¹•...)*")
        else: st.markdown(final_input)
    
    with st.chat_message("assistant"):
        with st.spinner("æ€è€ƒä¸­..."):
            # åœ¨è¿™é‡Œå·å·æŠŠå½“å‰æ—¶é—´ä¹Ÿå¡ç»™å¤§æ¨¡å‹ï¼Œè®©å®ƒåœ¨èŠå¤©ä¸­ä¹Ÿæœ‰æ—¶é—´è§‚å¿µ
            api_messages = [{"role": m["role"], "content": m["content"]} for m in st.session_state.messages]
            api_messages.append({"role": "system", "content": f"ã€éšè—æç¤ºã€‘å½“å‰æ—¶é—´æ˜¯ {datetime.datetime.now().strftime('%H:%M')}ã€‚"})
            
            res = client.chat.completions.create(
                model="deepseek-ai/DeepSeek-V3",
                messages=api_messages,
                temperature=0.75
            )
            reply = res.choices[0].message.content
            st.markdown(reply)
            
            audio_bytes = speak(reply, st.session_state.current_voice)
            st.session_state.clarity_score += 5
            
    st.session_state.messages.append({"role": "assistant", "content": reply, "audio": audio_bytes})
    st.rerun()